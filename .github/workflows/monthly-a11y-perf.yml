name: Monthly A11y + Lighthouse Reports

on:
  schedule:
    # Runs at 06:00 UTC on the 1st of every month
    - cron: "0 6 1 * *"
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: "Sitemap URL (xml or xml.gz). Used if single_url is empty. Leave empty to use env.SITEMAP_URL"
        required: false
        default: ""
      single_url:
        description: "Single URL to audit (overrides sitemap_url when provided)"
        required: false
        default: ""

permissions:
  contents: read

concurrency:
  group: monthly-a11y-perf
  cancel-in-progress: false

jobs:
  audit:
    runs-on: ubuntu-latest

    env:
      # Set your default sitemap here (can be overridden by workflow_dispatch input)
      SITEMAP_URL: "https://example.com/sitemap.xml"

      # Puppeteer/Chrome flags for GitHub runners
      CHROME_FLAGS: "--no-sandbox --disable-setuid-sandbox --disable-dev-shm-usage"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: "npm"

      - name: Install tooling (pa11y + lhci + xml parser)
        run: |
          npm ci || true
          npm install --no-save pa11y-ci pa11y-ci-reporter-html @lhci/cli fast-xml-parser

      - name: Resolve inputs/env
        id: vars
        run: |
          SITEMAP="${{ inputs.sitemap_url }}"
          if [ -z "$SITEMAP" ]; then SITEMAP="${SITEMAP_URL}"; fi

          SINGLE="${{ inputs.single_url }}"
          if [ -z "$SINGLE" ] && [ -n "${SINGLE_URL:-}" ]; then SINGLE="${SINGLE_URL}"; fi

          echo "sitemap=$SITEMAP" >> $GITHUB_OUTPUT
          echo "single_url=$SINGLE" >> $GITHUB_OUTPUT

      - name: Build URL list (sitemap or single url) -> urls.txt
        run: |
          node <<'NODE'
          const fs = require("fs");
          const zlib = require("zlib");
          const { XMLParser } = require("fast-xml-parser");

          const sitemapUrl = process.env.SITEMAP_URL_EFFECTIVE || "";
          const singleUrl = (process.env.SINGLE_URL_EFFECTIVE || "").trim();

          function assertHttpUrl(u) {
            try {
              const x = new URL(u);
              if (x.protocol !== "http:" && x.protocol !== "https:") throw new Error("Only http/https allowed");
            } catch {
              throw new Error(`Invalid URL: ${u}`);
            }
          }

          if (singleUrl) {
            assertHttpUrl(singleUrl);
            fs.writeFileSync("urls.txt", singleUrl + "\n", "utf8");
            fs.writeFileSync("urls.json", JSON.stringify([singleUrl], null, 2), "utf8");
            console.log(`Using single URL: ${singleUrl}`);
            process.exit(0);
          }

          if (!sitemapUrl) throw new Error("No single_url provided and no sitemap URL available.");

          const parser = new XMLParser({ ignoreAttributes: false, allowBooleanAttributes: true });

          async function fetchTextOrGunzip(url) {
            const res = await fetch(url, { redirect: "follow" });
            if (!res.ok) throw new Error(`Failed to fetch ${url} (${res.status})`);
            const buf = Buffer.from(await res.arrayBuffer());
            if (url.endsWith(".gz")) return zlib.gunzipSync(buf).toString("utf8");
            return buf.toString("utf8");
          }

          function toArray(x) {
            if (!x) return [];
            return Array.isArray(x) ? x : [x];
          }

          async function extractFromUrl(url) {
            const xml = await fetchTextOrGunzip(url);
            const data = parser.parse(xml);

            if (data.urlset && data.urlset.url) {
              const urls = toArray(data.urlset.url).map(u => u.loc).filter(Boolean).map(String);
              return { type: "urlset", urls };
            }

            if (data.sitemapindex && data.sitemapindex.sitemap) {
              const sitemaps = toArray(data.sitemapindex.sitemap).map(s => s.loc).filter(Boolean).map(String);
              return { type: "index", sitemaps };
            }

            throw new Error(`Unrecognized sitemap format at ${url}`);
          }

          (async () => {
            assertHttpUrl(sitemapUrl);

            const seen = new Set();
            const out = [];

            function addUrls(urls) {
              for (const u of urls) {
                if (!u) continue;
                try { assertHttpUrl(u); } catch { continue; }
                if (!seen.has(u)) {
                  seen.add(u);
                  out.push(u);
                }
              }
            }

            const root = await extractFromUrl(sitemapUrl);

            if (root.type === "urlset") {
              addUrls(root.urls);
            } else {
              for (const sm of root.sitemaps) {
                const child = await extractFromUrl(sm);
                if (child.type !== "urlset") continue;
                addUrls(child.urls);
              }
            }

            if (out.length === 0) throw new Error("Sitemap produced 0 valid URLs.");

            fs.writeFileSync("urls.txt", out.join("\n") + "\n", "utf8");
            fs.writeFileSync("urls.json", JSON.stringify(out, null, 2), "utf8");
            console.log(`Extracted ${out.length} URLs from sitemap`);
          })().catch(err => {
            console.error(err);
            process.exit(1);
          });
          NODE
        env:
          SITEMAP_URL_EFFECTIVE: ${{ steps.vars.outputs.sitemap }}
          SINGLE_URL_EFFECTIVE: ${{ steps.vars.outputs.single_url }}

      - name: Prepare report folders
        run: |
          mkdir -p reports/pa11y/html
          mkdir -p reports/lighthouse
          mkdir -p reports/meta

      - name: Create Pa11y config from urls.txt (reporters write files)
        run: |
          node <<'NODE'
          const fs = require("fs");

          const urls = fs.readFileSync("urls.txt", "utf8")
            .split("\n").map(s => s.trim()).filter(Boolean);

          const cfg = {
            urls,
            defaults: {
              timeout: 60000,
              chromeLaunchConfig: {
                args: (process.env.CHROME_FLAGS || "").split(" ").filter(Boolean),
              },
              reporters: [
                "cli",
                ["json", { fileName: "reports/pa11y/pa11y-report.json" }],
                ["pa11y-ci-reporter-html", { destination: "reports/pa11y/html" }]
              ]
            }
          };

          fs.writeFileSync(".pa11yci.json", JSON.stringify(cfg, null, 2), "utf8");
          console.log(`Pa11y config created with ${urls.length} URLs`);
          NODE
        env:
          CHROME_FLAGS: ${{ env.CHROME_FLAGS }}

      - name: Run Pa11y (sandbox fix + live logs)
        shell: bash
        run: |
          set +e
          export PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
          export PUPPETEER_EXECUTABLE_PATH=$(which chromium || which google-chrome || which chromium-browser || true)

          echo "Chromium path: $PUPPETEER_EXECUTABLE_PATH"
          echo "== Pa11y diagnostics =="
          node -v
          npx --yes pa11y-ci --version || true
          echo "======================="

          # In case no Chromium is installed, install one manually
          if [ -z "$PUPPETEER_EXECUTABLE_PATH" ]; then
            echo "Installing Chromium..."
            sudo apt-get update -y
            sudo apt-get install -y chromium-browser || sudo apt-get install -y chromium
            export PUPPETEER_EXECUTABLE_PATH=$(which chromium || which chromium-browser)
            echo "Chromium installed at $PUPPETEER_EXECUTABLE_PATH"
          fi

          echo "== Running Pa11y =="
          npx pa11y-ci -c .pa11yci.json --no-sandbox --threshold 999999 2>&1 | tee reports/pa11y/pa11y-run.log
          PA11Y_EXIT=${PIPESTATUS[0]}
          echo "Pa11y exit code: $PA11Y_EXIT"

          echo ""
          echo "Outputs:"
          ls -R reports/pa11y || true

          exit 0

      - name: Create Lighthouse CI config from urls.txt
        run: |
          node <<'NODE'
          const fs = require("fs");

          const urls = fs.readFileSync("urls.txt","utf8")
            .split("\n").map(s=>s.trim()).filter(Boolean);

          const chromeFlags = (process.env.CHROME_FLAGS || "").split(" ").filter(Boolean);

          const cfg = {
            ci: {
              collect: {
                url: urls,
                numberOfRuns: 1,
                settings: { chromeFlags }
              },
              upload: {
                target: "filesystem",
                outputDir: "reports/lighthouse"
              }
            }
          };

          fs.writeFileSync("lighthouserc.json", JSON.stringify(cfg, null, 2), "utf8");
          console.log(`LHCI config created with ${urls.length} URLs`);
          NODE
        env:
          CHROME_FLAGS: ${{ env.CHROME_FLAGS }}

      - name: Run Lighthouse CI (HTML + JSON artifacts)
        continue-on-error: true
        run: |
          npx lhci autorun --config=./lighthouserc.json || true

          if [ -d ".lighthouseci" ]; then
            mkdir -p reports/lighthouse/raw
            cp -R .lighthouseci/* reports/lighthouse/raw/ || true
          fi

      - name: Copy URL lists into reports for convenience
        run: |
          cp urls.txt reports/meta/urls.txt
          cp urls.json reports/meta/urls.json

      - name: Zip everything into a single file
        if: always()
        run: |
          TS="$(date -u +'%Y-%m')"
          ZIP="monthly-reports-${TS}.zip"
          zip -r "$ZIP" reports > /dev/null
          echo "ZIP_NAME=$ZIP" >> $GITHUB_ENV

      - name: Upload artifact (single zip)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monthly-a11y-lighthouse-reports
          path: ${{ env.ZIP_NAME }}
          retention-days: 30
